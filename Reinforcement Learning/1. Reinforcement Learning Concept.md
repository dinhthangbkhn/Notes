# Concept
## What is Reinforcement learning
Reinforcement learning is learning what to do—how to map situations to actions—so
as to maximize a numerical reward signal. The learner is not told which actions to
take, but instead must discover which actions yield the most reward by trying them. In
the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two
characteristics—trial-and-error search and delayed reward—are the two most important
distinguishing features of reinforcement learning.

The basic idea
is simply to capture the most important aspects of the real problem facing a learning
agent interacting over time with its environment to achieve a goal. A learning agent
must be able to sense the state of its environment to some extent and must be able to
take actions that affect the state. The agent also must have a goal or goals relating to
the state of the environment. Markov decision processes are intended to include just
these three aspects—sensation, action, and goal—in their simplest possible forms without
trivializing any of them. Any method that is well suited to solving such problems we
consider to be a reinforcement learning method.

## RL là gì?
Khác với **supervised learning**, **supervised learning** là quá trình learning từ một tập training set, mỗi training data
là thể hiện cho một trạng thái / hoàn cảnh với một đầu ra được định nghĩa trước (nhãn - *label*) nhưng nó là không phù hợp 
cho việc learning từ những tương tác (**interaction**) 

**Reinforcement learning** cũng khác với **Unsupervised learning**, RL cố gắng tối ưu sao cho phần thưởng (reward) được nhận là nhiều nhất thay vì đi tìm một cấu trúc ẩn bên trong dữ liệu như học không giám sát.

## Challenge of RL
One of the challenges that arise in reinforcement learning, and not in other kinds
of learning, is the trade-off between **exploration** and **exploitation**. To obtain a lot of
reward, a reinforcement learning agent must prefer actions that it has tried in the past
and found to be effective in producing reward. But to discover such actions, it has to
try actions that it has not selected before. The agent has to **exploit** what it has already
experienced in order to obtain reward, but it also has to **explore** in order to make better
action selections in the future.

# Example
A good way to understand reinforcement learning is to consider some of the examples
and possible applications that have guided its development.
- A master chess player makes a move. The choice is informed both by planning—
anticipating possible replies and counterreplies—and by immediate, intuitive judgments
of the desirability of particular positions and moves.
- An adaptive controller adjusts parameters of a petroleum refinery’s operation in
real time. The controller optimizes the yield/cost/quality trade-off↵ on the basis
of specified marginal costs without sticking strictly to the set points originally
suggested by engineers.
- A gazelle calf struggles to its feet minutes after being born. Half an hour later it is
running at 20 miles per hour.
- A mobile robot decides whether it should enter a new room in search of more trash
to collect or start trying to find its way back to its battery recharging station. It
makes its decision based on the current charge level of its battery and how quickly
and easily it has been able to find the recharger in the past.
- Phil prepares his breakfast. Closely examined, even this apparently mundane
activity reveals a complex web of conditional behavior and interlocking goal–subgoal
relationships: walking to the cupboard, opening it, selecting a cereal box, then
reaching for, grasping, and retrieving the box. Other complex, tuned, interactive
sequences of behavior are required to obtain a bowl, spoon, and milk carton. Each
step involves a series of eye movements to obtain information and to guide reaching
and locomotion. Rapid judgments are continually made about how to carry the
objects or whether it is better to ferry some of them to the dining table before
obtaining others. Each step is guided by goals, such as grasping a spoon or getting
to the refrigerator, and is in service of other goals, such as having the spoon to eat
with once the cereal is prepared and ultimately obtaining nourishment. Whether
he is aware of it or not, Phil is accessing information about the state of his body

These examples share features that are so basic that they are easy to overlook. All
involve interaction between an active decision-making agent and its environment, within
which the agent seeks to achieve a goal despite uncertainty about its environment. The
agent’s actions are permitted to affect the future state of the environment (e.g., the
next chess position, the level of reservoirs of the refinery, the robot’s next location and
the future charge level of its battery), thereby a↵ecting the actions and opportunities
available to the agent at later times. Correct choice requires taking into account indirect,
delayed consequences of actions, and thus may require foresight or planning.

